[working-directory: 'vllm']
install:
  uv pip install -r ../requirements.txt && \
  uv pip install -e .

[working-directory: 'vllm/benchmarks']
benchmark INPUT_LEN OUTPUT_LEN:
  python benchmark_one_concurrent_req.py \
    --model meta-llama/Llama-4-Scout-17B-16E-Instruct \
    --host $PROXY_HOST \
    --port $PROXY_PORT \
    --input-len {{INPUT_LEN}} \
    --output-len {{OUTPUT_LEN}}

[working-directory: 'vllm/benchmarks']
baseline INPUT_LEN OUTPUT_LEN:
  python benchmark_one_concurrent_req.py \
    --model meta-llama/Llama-4-Scout-17B-16E-Instruct \
    --host vllm-decoder-direct-service \
    --port 8200 \
    --input-len {{INPUT_LEN}} \
    --output-len {{OUTPUT_LEN}} \
    --num-requests 10

